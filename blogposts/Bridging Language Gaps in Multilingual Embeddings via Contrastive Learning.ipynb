{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridging Language Gaps in Multilingual Embeddings via Contrastive Learning by Jina AI (https://jina.ai/news/bridging-language-gaps-in-multilingual-embeddings-via-contrastive-learning/)\n",
    "\n",
    "## Key Highlights\n",
    "\n",
    "### The Language Gap in Multilingual Models\n",
    "- Multilingual embedding models often exhibit poor alignment between semantically similar phrases in different languages, resulting in a \"language gap.\"\n",
    "- This gap limits the effectiveness of cross-lingual applications like multilingual semantic search and translation.\n",
    "\n",
    "### Role of Training Approaches\n",
    "- **Masked Language Modeling (MLM):**\n",
    "  - Pretraining with masked tokens enables models to learn language patterns. However, embeddings often cluster by language rather than by shared semantics, contributing to language gap.\n",
    "- **Contrastive Learning:**\n",
    "  - This technique aligns embeddings of semantically similar text pairs across languages, pulling them closer in the shared embedding space.\n",
    "\n",
    "### Impact of Parallel Multilingual Data\n",
    "- Surprisingly, experiments with multilingual models (e.g., `jina-embeddings-v3`) showed that **explicit cross-lingual training data provided little to no improvement** for most language pairs.\n",
    "- Cross-language data appears to have more value for **low-resource languages** that are underrepresented in pretraining corpora, though further investigation is needed.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Multilingual Embedding Alignment:**\n",
    "   - Contrastive learning improves cross-lingual embedding alignment but does not always require explicit parallel data for most language pairs.\n",
    "\n",
    "2. **Parallel Data Value:**\n",
    "   - Explicit cross-lingual training data may be more beneficial for **low-resource languages** than for widely represented ones.\n",
    "\n",
    "3. **Future Exploration:**\n",
    "   - More research is needed to evaluate the role of cross-lingual data in fully trained models and larger-scale datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class for Data Preparation\n",
    "class ParallelDataset(Dataset):\n",
    "    def __init__(self, texts_a, texts_b, tokenizer, max_length=128):\n",
    "        self.texts_a = texts_a\n",
    "        self.texts_b = texts_b\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts_a)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_a = self.tokenizer(self.texts_a[idx], truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        tokenized_b = self.tokenizer(self.texts_b[idx], truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'input_ids_a': tokenized_a['input_ids'].squeeze(0),\n",
    "            'attention_mask_a': tokenized_a['attention_mask'].squeeze(0),\n",
    "            'input_ids_b': tokenized_b['input_ids'].squeeze(0),\n",
    "            'attention_mask_b': tokenized_b['attention_mask'].squeeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Contrastive Loss Function\n",
    "def contrastive_loss(embeddings_a, embeddings_b, temperature=0.07):\n",
    "    logits = torch.matmul(embeddings_a, embeddings_b.T) / temperature\n",
    "    labels = torch.arange(logits.size(0)).to(logits.device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "# Define Evaluation Metrics\n",
    "def cosine_similarity(embeddings_a, embeddings_b):\n",
    "    return F.cosine_similarity(embeddings_a, embeddings_b).mean().item()\n",
    "\n",
    "def spearman_correlation(embeddings_a, embeddings_b):\n",
    "    a = embeddings_a.cpu().detach().numpy()\n",
    "    b = embeddings_b.cpu().detach().numpy()\n",
    "    correlations = [spearmanr(a[i], b[i]).correlation for i in range(len(a))]\n",
    "    return sum(correlations) / len(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a parallel dataset (example: OPUS Books dataset)\n",
    "dataset = load_dataset(\"opus_books\", \"el-en\", split=\"train[:5000]\")\n",
    "texts_a = [pair['en'] for pair in dataset['translation'][:5000]]\n",
    "texts_b = [pair['el'] for pair in dataset['translation'][:5000]]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "parallel_dataset = ParallelDataset(texts_a, texts_b, tokenizer)\n",
    "dataloader = DataLoader(parallel_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "model = AutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "projection_head = nn.Sequential(\n",
    "    nn.Linear(model.config.hidden_size, 256),  # Reduced dimensions for faster computation\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training Cosine Similarity: 0.9976966977119446\n",
      "Pre-training Spearman Correlation: 0.6391859295076658\n"
     ]
    }
   ],
   "source": [
    "# Pre-training Evaluation\n",
    "sample_texts_a = [\"Two young girls are playing outside in a non-urban environment.\"]\n",
    "sample_texts_b = [\"Δύο νεαρά κορίτσια παίζουν έξω σε ενα μη αστικό περιβάλλον.\"]\n",
    "\n",
    "inputs_a = tokenizer(sample_texts_a, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs_b = tokenizer(sample_texts_b, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embed_a_pre = model(**inputs_a).last_hidden_state.mean(dim=1)\n",
    "    embed_b_pre = model(**inputs_b).last_hidden_state.mean(dim=1)\n",
    "\n",
    "cosine_sim_pre = cosine_similarity(embed_a_pre, embed_b_pre)\n",
    "spearman_corr_pre = spearman_correlation(embed_a_pre, embed_b_pre)\n",
    "print(f\"Pre-training Cosine Similarity: {cosine_sim_pre}\")\n",
    "print(f\"Pre-training Spearman Correlation: {spearman_corr_pre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [08:21<00:00, 23.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.8583420515060425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [08:20<00:00, 23.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 3.597190573101952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "optimizer = AdamW(list(model.parameters()) + list(projection_head.parameters()), lr=5e-5)\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    projection_head.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Process input batch\n",
    "        input_ids_a = batch['input_ids_a'].to(model.device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(model.device)\n",
    "        input_ids_b = batch['input_ids_b'].to(model.device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(model.device)\n",
    "\n",
    "        # Forward pass through model\n",
    "        outputs_a = model(input_ids_a, attention_mask=attention_mask_a).last_hidden_state.mean(dim=1)\n",
    "        outputs_b = model(input_ids_b, attention_mask=attention_mask_b).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Projection head\n",
    "        embeddings_a = projection_head(outputs_a)\n",
    "        embeddings_b = projection_head(outputs_b)\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        loss = contrastive_loss(embeddings_a, embeddings_b)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-training Cosine Similarity: 0.9605379104614258\n",
      "Post-training Spearman Correlation: 0.9579403955319539\n"
     ]
    }
   ],
   "source": [
    "# Post-training Evaluation\n",
    "with torch.no_grad():\n",
    "    embed_a_post = projection_head(model(**inputs_a).last_hidden_state.mean(dim=1))\n",
    "    embed_b_post = projection_head(model(**inputs_b).last_hidden_state.mean(dim=1))\n",
    "\n",
    "cosine_sim_post = cosine_similarity(embed_a_post, embed_b_post)\n",
    "spearman_corr_post = spearman_correlation(embed_a_post, embed_b_post)\n",
    "print(f\"Post-training Cosine Similarity: {cosine_sim_post}\")\n",
    "print(f\"Post-training Spearman Correlation: {spearman_corr_post}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "- Despite the small sample size and model, Spearman Correlation post-training improved significantly, increasing from 63% to 96%. This highlights the efficacy of the contrastive learning method in enhancing semantic alignment across languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge-distilled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
